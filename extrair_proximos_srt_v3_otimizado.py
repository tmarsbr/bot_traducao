#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Extrai SRT em ingl√™s de todos os v√≠deos da pasta proximos_para_traducao

OTIMIZA√á√ïES V3:
- FFmpeg Piping: √Åudio processado direto para mem√≥ria (sem arquivos tempor√°rios)
- dynaudnorm: Normaliza√ß√£o din√¢mica para √°udio com picos (superior ao loudnorm)
- Filtros de Confian√ßa: Usa no_speech_prob e avg_logprob para detectar alucina√ß√µes
- CPS: Verifica Caracteres Por Segundo para legibilidade
- VAD robusto para evitar transcri√ß√£o de sil√™ncios

ANTI-ALUCINA√á√ÉO:
- VAD (Voice Activity Detection): remove sil√™ncios antes de transcrever
- condition_on_previous_text=False: quebra loops de repeti√ß√£o
- no_speech_prob: descarta segmentos com alta probabilidade de sil√™ncio
- Thresholds rigorosos para evitar transcrever ru√≠do
"""

import os
import subprocess
import tempfile
from pathlib import Path
import time
import numpy as np
from faster_whisper import WhisperModel
from config import SUBTITLES_EN_DIR, SUBTITLES_OUTPUT_DIR, VIDEOS_OUTPUT_DIR, WHISPER_MODEL, MODELS_DIR

# Importar ferramentas do pipeline
try:
    from traduzir_com_gemini import traduzir_srt_gemini
    from embutir_legendas import embutir_legendas as embutir_legenda
except ImportError:
    print("‚ö†Ô∏è M√≥dulos traduzir_com_gemini ou embutir_legendas n√£o encontrados!")
    def traduzir_srt_gemini(*args): return False
    def embutir_legenda(*args): return False

# Configura√ß√µes
PASTA_ENTRADA = "proximos_para_traducao"
PASTA_SAIDA = str(SUBTITLES_EN_DIR)
MODELO = WHISPER_MODEL
DEVICE = "cpu"
COMPUTE_TYPE = "int8"

# Par√¢metros de Qualidade
LIMITE_NO_SPEECH = 0.6  # Se > 0.6, provavelmente √© sil√™ncio/ru√≠do
LIMITE_AVG_LOGPROB = -1.0  # Confian√ßa m√≠nima da transcri√ß√£o
LIMITE_CPS = 25  # Caracteres por segundo (acima √© dif√≠cil ler)

# Lista de alucina√ß√µes conhecidas do Whisper
ALUCINACOES_COMUNS = [
    "thank you for watching",
    "thanks for watching",
    "subscribe to our channel",
    "please subscribe",
    "see you next time",
    "stay tuned",
    "legendas por",
    "subtitles by",
    "amara.org",
    "transcribed by",
    "captioned by",
]

def eh_alucinacao_conhecida(texto):
    """Detecta frases comuns que o Whisper inventa em sil√™ncios."""
    texto_lower = texto.lower().strip()
    return any(alu in texto_lower for alu in ALUCINACOES_COMUNS)

def quebrar_legenda_netflix(texto, max_chars=42, max_linhas=2):
    """Quebra texto seguindo padr√£o Netflix de legendagem (max 42 chars/linha, 2 linhas)."""
    texto = texto.replace('\n', ' ').strip()
    palavras = texto.split()
    linhas = []
    linha_atual = ""
    
    for palavra in palavras:
        teste = f"{linha_atual} {palavra}".strip()
        if len(teste) <= max_chars:
            linha_atual = teste
        else:
            if linha_atual:
                linhas.append(linha_atual)
            linha_atual = palavra
    
    if linha_atual:
        linhas.append(linha_atual)
    
    return "\n".join(linhas[:max_linhas])

def carregar_audio_via_pipe(video_path):
    """
    Usa FFmpeg para extrair, normalizar e enviar o √°udio diretamente para a mem√≥ria (Pipe).
    
    Aplica:
    - dynaudnorm: Normaliza√ß√£o din√¢mica (√≥timo para picos/sussurros)
    - arnndn: Redu√ß√£o de ru√≠do com IA (se dispon√≠vel, sen√£o usa afftdn)
    - highpass/lowpass: Remove frequ√™ncias fora da voz humana
    - ar=16000: Taxa de amostragem nativa do Whisper
    - ac=1: Mono (Whisper n√£o precisa de est√©reo)
    - f=s16le: Formato PCM 16-bit raw
    
    Returns:
        numpy.array: √Åudio em float32 normalizado, ou None se falhar
    """
    # Filtro otimizado com fallback
    # Tenta arnndn (melhor), se falhar usa afftdn
    filtro_principal = (
        "arnndn=m=models/rnn.rnnn,"  # Noise reduction com IA (pode n√£o estar dispon√≠vel)
        "dynaudnorm=f=150:g=15:p=0.9,"  # Normaliza√ß√£o din√¢mica
        "highpass=f=200,"  # Remove graves (n√£o-voz)
        "lowpass=f=3000"   # Remove agudos (chiados)
    )
    
    # Filtro fallback (sem arnndn)
    filtro_fallback = (
        "afftdn=nr=20:nf=-30,"  # Noise reduction FFT
        "dynaudnorm=f=150:g=15:p=0.9,"
        "highpass=f=200,"
        "lowpass=f=3000"
    )
    
    comando = [
        "ffmpeg",
        "-i", video_path,
        "-af", filtro_fallback,  # Usar fallback por padr√£o (mais compat√≠vel)
        "-ar", "16000",
        "-ac", "1",
        "-f", "s16le",
        "-vn",
        "-"
    ]
    
    try:
        processo = subprocess.Popen(
            comando,
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL
        )
        
        dados_raw, _ = processo.communicate(timeout=300)
        
        if processo.returncode != 0 or len(dados_raw) == 0:
            return None
        
        # Converte bytes para array NumPy float32 (formato esperado pelo Whisper)
        audio_array = np.frombuffer(dados_raw, np.int16).flatten().astype(np.float32) / 32768.0
        
        return audio_array
        
    except subprocess.TimeoutExpired:
        processo.kill()
        return None
    except Exception as e:
        print(f"    ‚ö†Ô∏è Erro no pipe: {str(e)[:50]}")
        return None

def format_timestamp(seconds):
    """Converte segundos para formato SRT HH:MM:SS,mmm"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

def transcrever_audio_otimizado(audio_array, model):
    """
    Transcreve √°udio com filtros de confian√ßa integrados.
    
    Retorna apenas segmentos de alta qualidade:
    - Filtra por no_speech_prob (probabilidade de sil√™ncio)
    - Filtra por avg_logprob (confian√ßa da transcri√ß√£o)
    - Detecta alucina√ß√µes conhecidas
    - Verifica CPS (legibilidade)
    """
    try:
        vad_parameters = {
            "threshold": 0.5,
            "min_speech_duration_ms": 250,
            "min_silence_duration_ms": 500
        }
        
        segments, info = model.transcribe(
            audio_array,
            language="en",
            beam_size=5,
            without_timestamps=False,
            condition_on_previous_text=False,
            vad_filter=True,
            vad_parameters=vad_parameters,
            no_speech_threshold=0.4,
            log_prob_threshold=-0.9,
            compression_ratio_threshold=2.4,
            word_timestamps=True,
        )
        
        # Filtrar segmentos por qualidade
        segmentos_filtrados = []
        stats = {
            'total': 0,
            'no_speech': 0,
            'low_prob': 0,
            'alucinacao': 0,
            'cps_alto': 0,
            'aprovados': 0
        }
        
        for segment in segments:
            stats['total'] += 1
            
            # Filtro 1: Probabilidade de N√£o-Fala
            if segment.no_speech_prob > LIMITE_NO_SPEECH:
                stats['no_speech'] += 1
                continue
            
            # Filtro 2: Confian√ßa da transcri√ß√£o
            if segment.avg_logprob < LIMITE_AVG_LOGPROB:
                stats['low_prob'] += 1
                continue
            
            # Filtro 3: Alucina√ß√µes conhecidas
            texto = segment.text.strip()
            if eh_alucinacao_conhecida(texto):
                stats['alucinacao'] += 1
                continue
            
            # Filtro 4: CPS (apenas alerta, n√£o bloqueia)
            duracao = segment.end - segment.start
            if duracao > 0:
                cps = len(texto) / duracao
                if cps > LIMITE_CPS:
                    stats['cps_alto'] += 1
            
            stats['aprovados'] += 1
            segmentos_filtrados.append({
                'start': segment.start,
                'end': segment.end,
                'text': texto,
                'no_speech_prob': segment.no_speech_prob,
                'avg_logprob': segment.avg_logprob
            })
        
        # Log de estat√≠sticas
        if stats['total'] > 0:
            filtrados = stats['no_speech'] + stats['low_prob'] + stats['alucinacao']
            if filtrados > 0:
                print(f"(filtrou {filtrados}: {stats['no_speech']} sil√™ncio, "
                      f"{stats['low_prob']} baixa conf., {stats['alucinacao']} aluc.)", 
                      end=" ", flush=True)
        
        return segmentos_filtrados
        
    except Exception as e:
        print(f"    ‚ùå Transcri√ß√£o falhou: {str(e)[:60]}")
        return None

def salvar_srt(segments, output_path):
    """Salva segmentos em SRT com formata√ß√£o Netflix"""
    contador = 0
    with open(output_path, 'w', encoding='utf-8') as f:
        for segment in segments:
            texto = quebrar_legenda_netflix(segment['text'], max_chars=42, max_linhas=2)
            
            contador += 1
            start = format_timestamp(segment['start'])
            end = format_timestamp(segment['end'])
            f.write(f"{contador}\n{start} --> {end}\n{texto}\n\n")
    
    return contador

def extrair_srt_otimizado(video_path, model):
    """Extrai SRT de um v√≠deo usando piping otimizado"""
    nome_video = Path(video_path).stem
    srt_saida = os.path.join(PASTA_SAIDA, f"{nome_video}_EN.srt")
    
    # Pular se j√° existe
    if os.path.exists(srt_saida):
        print(f"  ‚úì J√° existe")
        return nome_video, True
    
    try:
        print(f"  1Ô∏è‚É£ Processando √°udio (pipe+filtros)...", end=" ", flush=True)
        audio_array = carregar_audio_via_pipe(video_path)
        if audio_array is None:
            print("‚ùå FFmpeg falhou")
            return nome_video, False
        print("‚úì")
        
        print(f"  2Ô∏è‚É£ Transcrevendo com filtros...", end=" ", flush=True)
        segments = transcrever_audio_otimizado(audio_array, model)
        if segments is None or len(segments) == 0:
            print("‚ùå Whisper falhou ou nenhum segmento v√°lido")
            return nome_video, False
        print("‚úì")
        
        print(f"  3Ô∏è‚É£ Salvando SRT...", end=" ", flush=True)
        contador = salvar_srt(segments, srt_saida)
        print(f"‚úì ({contador} legendas)")
        
        return nome_video, True
        
    except Exception as e:
        print(f"‚ùå {str(e)[:60]}")
        return nome_video, False

# Carregar modelo uma √∫nica vez
print("‚è≥ Carregando modelo Whisper...")
model = WhisperModel(MODELO, device=DEVICE, compute_type=COMPUTE_TYPE, download_root=str(MODELS_DIR))
print("‚úÖ Modelo carregado!\n")

def main():
    os.makedirs(PASTA_SAIDA, exist_ok=True)
    
    # Listar todos os v√≠deos
    todos_videos = sorted([
        os.path.join(PASTA_ENTRADA, f) 
        for f in os.listdir(PASTA_ENTRADA) 
        if f.lower().endswith('.mp4')
    ])
    
    print(f"üé¨ Encontrados {len(todos_videos)} v√≠deos na pasta\n")
    
    # Filtrar v√≠deos que j√° t√™m SRT existente
    videos = []
    ja_existentes = 0
    for video in todos_videos:
        nome_video = Path(video).stem
        srt_path = os.path.join(PASTA_SAIDA, f"{nome_video}_EN.srt")
        if os.path.exists(srt_path):
            ja_existentes += 1
        else:
            videos.append(video)
    
    if ja_existentes > 0:
        print(f"‚úÖ {ja_existentes} v√≠deo(s) j√° t√™m SRT - pulando")
    
    if len(videos) == 0:
        print(f"\nüéâ Todos os v√≠deos j√° t√™m SRT extra√≠do!")
        return
    
    print(f"üìã {len(videos)} v√≠deo(s) para processar\n")
    print(f"üöÄ OTIMIZA√á√ïES ATIVAS:")
    print(f"   ‚Ä¢ FFmpeg Piping (sem arquivos tempor√°rios)")
    print(f"   ‚Ä¢ dynaudnorm (normaliza√ß√£o din√¢mica)")
    print(f"   ‚Ä¢ Filtros de Confian√ßa (no_speech_prob, avg_logprob)")
    print(f"   ‚Ä¢ Detec√ß√£o de alucina√ß√µes conhecidas")
    print(f"   ‚Ä¢ Formata√ß√£o Netflix (42 chars/linha)\n")
    
    print(f"üöÄ Iniciando Pipeline Sequencial (Extrair -> Traduzir -> Embutir)\n")
    
    total_videos = len(videos)
    sucessos_finais = 0
    
    for i, video in enumerate(videos, 1):
        nome_completo = Path(video).name
        nome_video = Path(video).stem
        print(f"[{i}/{total_videos}] üé¨ Processando: {nome_completo}")
        
        # 1. Extrair com otimiza√ß√µes
        _, sucesso_extracao = extrair_srt_otimizado(video, model)
        if not sucesso_extracao:
            print(f"  ‚è≠Ô∏è Pulando etapas seguintes (Extra√ß√£o falhou)\n")
            continue
            
        srt_en_path = os.path.join(PASTA_SAIDA, f"{nome_video}_EN.srt")
        
        # 2. Traduzir
        srt_pt_path = os.path.join(SUBTITLES_OUTPUT_DIR, f"{nome_video}_PT.srt")
        print(f"  ü§ñ Traduzindo (Gemini)...", end=" ", flush=True)
        sucesso_traducao = traduzir_srt_gemini(srt_en_path, srt_pt_path)
        if sucesso_traducao:
            print("‚úì")
        else:
            print("‚ùå Falha na tradu√ß√£o")
        
        # 3. Embutir (apenas se tradu√ß√£o foi bem-sucedida)
        if sucesso_traducao and os.path.exists(srt_pt_path):
            video_final_path = os.path.join(VIDEOS_OUTPUT_DIR, f"{nome_video}_PT.mp4")
            print(f"  üìΩÔ∏è Embutindo legenda...", end=" ", flush=True)
            sucesso_embed = embutir_legenda(video, srt_pt_path, video_final_path)
            if sucesso_embed:
                print("‚úì")
                print(f"  ‚ú® V√≠deo finalizado: {video_final_path}")
                sucessos_finais += 1
            else:
                print("‚ùå Falha ao embutir")
        
        # 4. Cooldown
        if i < total_videos:
            print(f"  üí§ Aguardando 30s para o pr√≥ximo v√≠deo...\n")
            time.sleep(30)
    
    print("\n" + "="*70)
    print(f"üèÅ Pipeline Finalizado: {sucessos_finais}/{total_videos} v√≠deos completados!")
    print("="*70 + "\n")

if __name__ == "__main__":
    main()
